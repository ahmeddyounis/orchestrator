# L3 Orchestration: Automated Diagnosis and Repair

L3 is the most advanced orchestration mode, designed to tackle complex problems that require deep analysis and multiple attempts to solve. It uses a "best-of-N" approach to generate and evaluate multiple candidate solutions, selecting the one that best achieves the user's objective while still passing all required tests.

## When to Use L3

Use L3 mode (`--think L3`) when:

- A problem is complex and likely has multiple potential solutions.
- A test is consistently failing, and the root cause is not immediately obvious.
- You want the orchestrator to explore different approaches and choose the most effective one, balancing correctness with the original goal.
- Simpler modes (L1/L2) have failed to produce a satisfactory solution.

L3 is not recommended for simple, straightforward tasks where the solution path is clear. It consumes more time and resources (i.e., tokens) than other modes due to its multi-candidate generation and evaluation process.

## How It Works

The L3 process follows a structured, multi-stage workflow:

### 1. Best-of-N Candidate Generation

Instead of generating a single solution, L3 creates multiple (`N`) independent candidates. The number of candidates can be configured with the `--best-of <N>` flag (e.g., `--best-of 3`). Each candidate is developed in isolation, with its own independent context and memory.

### 2. Objective-First Evaluation (Reviewer)

Once all candidates are generated, the **Reviewer** agent assesses them. The Reviewer's primary goal is to determine how well each candidate's proposed changes align with the original user objective. It ranks the candidates based on this alignment, producing a preliminary order of preference. The Reviewer does _not_ run tests; its focus is purely on semantic and logical correctness relative to the prompt.

### 3. Verification and Diagnosis (Judge)

After the Reviewer ranks the candidates, the orchestrator runs the project's test suite against each one.

- **If a candidate passes all tests**, it is considered valid.
- **If a candidate fails tests**, the **Judge** agent is invoked. The Judge performs a targeted, bounded diagnosis to understand the _reason_ for the failure. It analyzes the test output and the candidate's code to determine if the failure is related to the proposed change or an unrelated issue.

The Judge's role is not to fix the code but to provide a structured verdict on the nature of the failure. This helps the orchestrator make an informed final decision.

### 4. Final Selection

The final ranking is determined by a combination of factors:

1.  **Test Passage**: Candidates that pass all tests are always preferred over those that fail.
2.  **Reviewer Ranking**: For candidates that pass tests, the Reviewer's ranking (based on the original objective) is used as the primary tie-breaker.
3.  **Judge's Verdict**: For failing candidates, the Judge's analysis provides additional data but does not override test failures.

The orchestrator selects the top-ranked, valid candidate as the final proposed solution.

## Cost Control and Safeguards

L3 orchestration can be significantly more expensive than other modes. Here are the recommended settings to keep costs under control, configured in `.orchestrator/config.json`:

```json
{
  "orchestration": {
    "l3": {
      "maxBestOf": 5,
      "maxRounds": 1,
      "maxL3Retries": 1
    }
  }
}
```

- `"maxBestOf": 5"`: Limits the maximum number of candidates (`--best-of N`) that can be generated in a single run. This is a critical safety rail to prevent accidental high costs.
- `"maxRounds": 1"`: L3 supports multi-round sessions where it can iteratively refine solutions. For most use cases, keeping this to `1` is sufficient and prevents run-away processes.
- `"maxL3Retries": 1"`: If an L3 run fails to produce a valid candidate, this setting controls how many times it can automatically retry the entire process.

## Example Session: Fixing a Failing Test

Imagine you have a failing test in `utils.test.ts`.

**Command:**

```bash
$ orchestrator --think L3 --best-of 3 --prompt "Fix the failing test in utils.test.ts"
```

The orchestrator will:

1.  Generate 3 independent candidate solutions to fix the test.
2.  The **Reviewer** will analyze each candidate and rank them based on how well they address the fix.
3.  Each candidate will be tested.
4.  The orchestrator will select the best candidate that passes the tests.

### Artifacts

After an L3 run, you can find the results in the `.runs` directory. Each run has a unique ID, and inside you will find:

- `.runs/<run-id>/l3/`: The main directory for L3 artifacts.
  - `candidate-0/`, `candidate-1/`, `candidate-2/`: Subdirectories for each candidate, containing:
    - `patch.diff`: The code changes proposed by that candidate.
    - `trace.json`: A log of the agent's thought process.
  - `review.json`: The Reviewer's ranking and analysis of all candidates.
  - `final_selection.json`: The final decision, including test results and the Judge's verdicts (if any).

By inspecting these artifacts, you can understand why a particular solution was chosen and review the alternative approaches that were considered.
